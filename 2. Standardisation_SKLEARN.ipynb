{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <span style='color:SteelBlue'>Normalisation et standardisation avec Scikit-Learn</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <span style='background:GreenYellow'>Normalisation avec MinMaxScaler</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> - Plutôt que d’appliquer directement la fonction de normalisation, il est préférable d’utiliser une fonctionnalité de Scikit-Learn appelée __Transformer API__, qui vous permettra __d’ajuster (fit)__ une étape de preprocessing en utilisant __les données d’entraînement.__\n",
    "> - Ainsi lorsque la normalisation, par exemple, sera appliquée à de nouvelles données (par exemple celles de test), elle utilisera __les mêmes moyennes et écart-types sauvegardés.__\n",
    "> - Pour créer cette étape de preprocessing ‘ajustée’ il suffit d’utiliser la fonction Min MaxScaler puis de l’ajuster grâce aux données d’entraînement. \n",
    "> - Enfin pour l’appliquer à un tableau de données par la suite il faudra simplement lui appliquer __scaler.transform().__\n",
    "> __fit_transform() :__\n",
    "> - la méthode fit_transform() est essentiellement la combinaison de la méthode fit et de la méthode transform, elle est équivalente à __fit().transform()__. \n",
    "> - Cette méthode effectue un __ajustement et une transformation sur les données d'entrée en une seule fois__ et convertit les points de données. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données\n",
      "[[1.0e+02 1.0e-03]\n",
      " [8.0e+00 5.0e-02]\n",
      " [5.0e+01 5.0e-03]\n",
      " [8.8e+01 7.0e-02]\n",
      " [4.0e+00 1.0e-01]]\n",
      "\n",
      "Données normalisées\n",
      "[[1.         0.        ]\n",
      " [0.04166667 0.49494949]\n",
      " [0.47916667 0.04040404]\n",
      " [0.875      0.6969697 ]\n",
      " [0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Exemple de normalisation\n",
    "from numpy import asarray\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Définissons les données\n",
    "data = asarray([[100, 0.001],\n",
    "                [8, 0.05],\n",
    "                [50, 0.005],\n",
    "                [88, 0.07],\n",
    "                [4, 0.1]])\n",
    "print(\"Données\")\n",
    "print(data)\n",
    "print(\"\")\n",
    "# Fonction min max scaler\n",
    "scaler = MinMaxScaler()\n",
    "# On transforme les données\n",
    "scaled = scaler.fit_transform(data)\n",
    "print(\"Données normalisées\")\n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouvelles données\n",
      "[[9.7e+01 5.0e-03]\n",
      " [6.0e+00 6.0e-02]\n",
      " [4.0e+01 3.0e-03]]\n",
      "\n",
      "Nouvelles données normalisées\n",
      "[[0.96875    0.04040404]\n",
      " [0.02083333 0.5959596 ]\n",
      " [0.375      0.02020202]]\n"
     ]
    }
   ],
   "source": [
    "# Normalisation\n",
    "# Nouvelles données\n",
    "data_new = asarray([[97, 0.005],\n",
    "                    [6, 0.06],\n",
    "                    [40, 0.003]])\n",
    "print(\"Nouvelles données\")\n",
    "print(data_new)\n",
    "print(\"\")\n",
    "\n",
    "# Attention pas de fit!\n",
    "new_scaled = scaler.transform(data_new)\n",
    "print(\"Nouvelles données normalisées\")\n",
    "print(new_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <span style='background:GreenYellow'>Standardisation (normalisation standard) avec StandardScaler</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données\n",
      "[[1.0e+02 1.0e-03]\n",
      " [8.0e+00 5.0e-02]\n",
      " [5.0e+01 5.0e-03]\n",
      " [8.8e+01 7.0e-02]\n",
      " [4.0e+00 1.0e-01]]\n",
      "\n",
      "Données normalisées\n",
      "[[ 1.26398112 -1.16389967]\n",
      " [-1.06174414  0.12639634]\n",
      " [ 0.         -1.05856939]\n",
      " [ 0.96062565  0.65304778]\n",
      " [-1.16286263  1.44302493]]\n"
     ]
    }
   ],
   "source": [
    "# Exemple de standardisation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Définissons les données\n",
    "data = asarray([[100, 0.001],\n",
    "                [8, 0.05],\n",
    "                [50, 0.005],\n",
    "                [88, 0.07],\n",
    "                [4, 0.1]])\n",
    "print(\"Données\")\n",
    "print(data)\n",
    "print(\"\")\n",
    "# Fonction min max scaler\n",
    "scaler = StandardScaler()\n",
    "# On transforme les données\n",
    "scaled = scaler.fit_transform(data)\n",
    "print(\"Données normalisées\")\n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.2204460492503132e-17, 1.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardisation\n",
    "# Moyenne et écart-type\n",
    "scaled.mean(), scaled.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouvelles données\n",
      "[[9.7e+01 5.0e-03]\n",
      " [6.0e+00 6.0e-02]\n",
      " [4.0e+01 3.0e-03]]\n",
      "\n",
      "Nouvelles données standardisées\n",
      "[[ 1.18814225 -1.05856939]\n",
      " [-1.11230338  0.38972206]\n",
      " [-0.25279622 -1.11123453]]\n"
     ]
    }
   ],
   "source": [
    "# Standardisation\n",
    "# Nouvelles données\n",
    "data_new = asarray([[97, 0.005],\n",
    "                    [6, 0.06],\n",
    "                    [40, 0.003]])\n",
    "print(\"Nouvelles données\")\n",
    "print(data_new)\n",
    "print(\"\")\n",
    "\n",
    "# Attention pas de fit!\n",
    "new_scaled = scaler.transform(data_new)\n",
    "print(\"Nouvelles données standardisées\")\n",
    "print(new_scaled)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
